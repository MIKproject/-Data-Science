# -*- coding: utf-8 -*-
"""12.2 üç∏üçß –í–∑–±–æ–ª—Ç–∞—Ç—å, –Ω–æ –Ω–µ —Å–º–µ—à–∏–≤–∞—Ç—å. –ü—Ä–∞–∫—Ç–∏–∫–∞.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QiRlJbM08ry5ILtt1RFZr4jr5LTQjjkn

# **–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —É—Ä–æ–∫ –ø–æ –±–ª–µ–Ω–¥–∏–Ω–≥—É –∏ —Å—Ç–µ–∫–∏–Ω–≥—É**

## **TASK DESCRIPTION**

–î–∞—Ç–∞—Å–µ—Ç—ã:
- train: https://raw.githubusercontent.com/a-milenkin/Competitive_Data_Science/main/data/quickstart_train.csv

- test: https://raw.githubusercontent.com/a-milenkin/Competitive_Data_Science/main/data/quickstart_test.csv

–†–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–π —Ç–∞—Ä–≥–µ—Ç: —Å—Ç–æ–ª–±–µ—Ü target_reg , –º–µ—Ç—Ä–∏–∫–∞ RMSE.

–î–ª—è –±–ª–µ–Ω–¥–∏–Ω–≥–∞ –∏ —Å—Ç–µ–∫–∏–Ω–≥–∞ –≤—ã–±—Ä–∞—Ç—å 3 –º–æ–¥–µ–ª–∏:
- –õ—é–±—ã–µ 2 –±—É—Å—Ç–∏–Ω–≥–∞ –Ω–∞ –≤–∞—à –≤—ã–±–æ—Ä.
- –õ—é–±—É—é –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å (–Ω–µ –±—É—Å—Ç–∏–Ω–≥).
- –î–ª—è —Å—Ç–µ–∫–∏–Ω–≥–∞ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –º–µ—Ç–∞–º–æ–¥–µ–ª–∏ –∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ –≤–∑—è—Ç—å –ø—Ä–æ—Å—Ç–æ–π –∞–ª–≥–æ—Ä–∏—Ç–º (—Ä–µ–≥—Ä–µ—Å—Å–∏—é, KNN –∏ —Ç. –ø.).
- –ù–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –≤—ã–±–∏—Ä–∞—Ç—å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ —Ç—Ä–æ–π–∫–∏, –º–æ–∂–Ω–æ –¥–ª—è —Å—Ç–µ–∫–∏–Ω–≥–∞ –æ–¥–Ω–∏, –¥–ª—è –±–ª–µ–Ω–¥–∏–Ω–≥–∞ –¥—Ä—É–≥–∏–µ 3. –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª–∏ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –ø—Ä–∞–∫—Ç–∏–∫–∏.

–î–∞–ª–µ–µ —Ä–µ–∞–ª–∏–∑—É–µ–º –±–ª–µ–Ω–¥–∏–Ω–≥ –∏ —Å—Ç–µ–∫–∏–Ω–≥ –∏ –¥–µ–ª–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ:
- –º–æ–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –æ—Ç –±–ª–µ–Ω–¥–∏–Ω–≥–∞;
- –º–æ–∂–Ω–æ –æ—Ç —Å—Ç–µ–∫–∏–Ω–≥–∞;
- –º–æ–∂–Ω–æ –∞–Ω—Å–∞–º–±–ª—å –±–ª–µ–Ω–¥–∏–Ω–≥–∞ –∏ —Å—Ç–µ–∫–∏–Ω–≥–∞;
- –º–æ–∂–Ω–æ –±–ª–µ–Ω–¥–∏–Ω–≥ –±–ª–µ–Ω–¥–∏–Ω–≥–∞ –∏ —Å—Ç–µ–∫–∏–Ω–≥–∞ —Å –≤–µ—Å–∞–º–∏ üôà.

–ì–ª–∞–≤–Ω–æ–µ –≤ —ç—Ç–æ–º –∑–∞–¥–∞–Ω–∏–∏ - –±–æ–ª—å—à–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å, –∑–∞—Å—ã–ª–∞—Ç—å —Å–∞–±–º–∏—à–µ–Ω—ã –≤ —Å–ª–µ–¥—É—é—â–∏–π —à–∞–≥, –ø—Ä–æ–≤–µ—Ä—è—Ç—å —Å–∫–æ—Ä, –ø–æ–¥–±–∏—Ä–∞—Ç—å –º–æ–¥–µ–ª–∏ –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –≤–µ—Å–∞ –¥–ª—è —Å–º–µ—à–∏–≤–∞–Ω–∏—è. –í –æ–±—â–µ–º, –ø–æ—á—É–≤—Å—Ç–≤–æ–≤–∞—Ç—å –∞—Ç–º–æ—Å—Ñ–µ—Ä—É —á–µ–º–ø–∏–æ–Ω–∞—Ç–∞ –∏ –¥–æ–ª–±–∏—Ç—å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –ø–æ–¥–±–∏—Ä–∞—Ç—å –º–æ–¥–µ–ª–∏, –≤–µ—Å–∞ –∏ —Å–º–æ—Ç—Ä–µ—Ç—å, –∫–∞–∫ –ø–∞–¥–∞–µ—Ç —Ü–∏—Ñ–µ—Ä–∫–∞ ü§§. –¢–æ–ª—å–∫–æ —É –Ω–∞—Å –ø–æ–ø—ã—Ç–∫–∏ –Ω–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω—ã 5 –≤ –¥–µ–Ω—å, –∏ –º–æ–∂–Ω–æ –æ—Ç–æ—Ä–≤–∞—Ç—å—Å—è –ø–æ –ø–æ–ª–Ω–æ–π.

–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —É—Å–ª–æ–≤–∏—è:
- –û—Å–æ–±–µ–Ω–Ω–æ –Ω–∏—á–µ–º –≤–∞—Å –Ω–µ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º, –º–æ–∂–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –≤—Å–µ–º –∞—Ä—Å–µ–Ω–∞–ª–æ–º: –ø–æ–¥–±–∏—Ä–∞—Ç—å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ Optuna, –≥–µ–Ω–µ—Ä–∏—Ç—å\–æ—Ç–±–∏—Ä–∞—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —Ç. –ø. –ù–æ –∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–∏—Ç—å—Å—è –Ω–∞ –æ—Å–≤–æ–µ–Ω–∏–∏ –±–ª–µ–Ω–¥–∏–Ω–≥–∞ —Å–æ —Å—Ç–µ–∫–∏–Ω–≥–æ–º –∏ –ø–æ–¥–±–æ—Ä–æ–º –º–æ–¥–µ–ª–µ–π –¥–ª—è –Ω–∏—Ö.
- –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ submission.csv —Ç–æ–ª—å–∫–æ –∫–æ–ª–æ–Ω–∫–∏ car_id –∏ target_reg.
–í –∫–æ–Ω—Ü–µ –¥–æ–±–∞–≤–ª—è–µ–º –≤—ã–≤–æ–¥—ã –æ –ø—Ä–æ–¥–µ–ª–∞–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ö, —á—Ç–æ –∑–∞—à–ª–æ\–Ω–µ –∑–∞—à–ª–æ.
–ñ–µ–ª–∞—Ç–µ–ª—å–Ω–æ, —á—Ç–æ–±—ã –Ω–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≤—Å–µ–≥–æ –Ω–æ—É—Ç–±—É–∫–∞ —É—Ö–æ–¥–∏–ª–æ –Ω–µ –±–æ–ª–µ–µ 30 –º–∏–Ω.

–§–æ—Ä–º–∞—Ç —Å–¥–∞—á–∏:
- –í —Ç–µ–∫—Å—Ç–æ–≤–æ–º –ø–æ–ª–µ –≤–≤–æ–¥–∏–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –æ–Ω–ª–∞–π–Ω-–≤–µ—Ä—Å–∏—é jupyter –Ω–æ—É—Ç–±—É–∫–∞ (Google–°olab, Kaggle) —Å –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã–º –∑–∞–¥–∞–Ω–∏–µ–º –∏ –ø—É–±–ª–∏—á–Ω—É—é —Å—Å—ã–ª–∫—É –Ω–∞ —Ñ–∞–π–ª—Ö–æ—Å—Ç–∏–Ω–≥ (GoogleDrive, –Ø–Ω–¥–µ–∫—Å–î–∏—Å–∫ –∏–ª–∏ –¥—Ä.) —Å –æ—Ä–∏–≥–∏–Ω–∞–ª–æ–º –Ω–æ—É—Ç–±—É–∫–∞ (–ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –æ–Ω–∏ —Ä–∞–±–æ—Ç–∞—é—Ç).
- –í–æ –≤–ª–æ–∂–µ–Ω–∏–µ –¥–æ–±–∞–≤–ª—è–µ–º —Ç–æ—Ç –∂–µ –Ω–æ—É—Ç–±—É–∫, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ pdf —Å –∞—É—Ç–ø—É—Ç–∞–º–∏ –≤—Å–µ—Ö —è—á–µ–µ–∫ (–Ω–µ –±–æ–ª–µ–µ 5 Mb).
- –í —Å–ª–µ–¥—É—é—â–µ–º —à–∞–≥–µ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –ø–æ–ª—É—á–∏–≤—à–∏–π—Å—è CSV-—Ñ–∞–π–ª —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏. –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–≥–æ –∫–∞–∫ –∞–Ω–∞–ª–æ–≥ –ª–∏–¥–µ—Ä–±–æ—Ä–¥–∞, —á—Ç–æ–±—ã —É–∑–Ω–∞–≤–∞—Ç—å —Å–∫–æ—Ä –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π.

## **IMPORT LIBRARIES AND LOAD DATA**
"""

!pip install optuna xgboost lightgbm catboost shap phik -q

import pandas as pd
import numpy as np
import seaborn as sns

from sklearn.model_selection import KFold, train_test_split
from sklearn.preprocessing import  StandardScaler, OrdinalEncoder
from sklearn.pipeline import make_pipeline
from sklearn.compose import make_column_transformer, make_column_selector
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, StackingRegressor
from sklearn.metrics import mean_squared_error

from catboost import CatBoostRegressor, Pool
from lightgbm import LGBMRegressor, Dataset
import lightgbm as lgb
from xgboost import XGBRegressor, DMatrix
import xgboost as xgb

from tqdm import tqdm
import matplotlib.pyplot as plt
import shap
import phik
from phik.report import plot_correlation_matrix
from phik import report
import optuna

import warnings
warnings.filterwarnings("ignore")

train = pd.read_csv("https://raw.githubusercontent.com/a-milenkin/Competitive_Data_Science/main/data/quickstart_train.csv")
test = pd.read_csv("https://raw.githubusercontent.com/a-milenkin/Competitive_Data_Science/main/data/quickstart_test.csv")
car_id = pd.read_csv("https://raw.githubusercontent.com/a-milenkin/Competitive_Data_Science/main/data/quickstart_test.csv")['car_id']

"""## **EDA**"""

train.info()

test.info()

"""–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç, –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é —Å–¥–µ–ª–∞–µ–º –ø–æ—Å–ª–µ feature engineering

## **FEATURE ENGINEERING**
"""

# –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–æ–≤–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞, –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–æ–∑—Ä–∞—Å—Ç–∞ –∞–≤—Ç–æ–º–æ–±–∏–ª—è
train['age_of_car'] = train['year_to_work'] - train['year_to_start']
test['age_of_car'] = test['year_to_work'] - test['year_to_start']

drop_cols = ['car_id', 'target_reg', 'target_class']
targets = ['target_reg']
cat_features = ['car_type', 'fuel_type', 'model']

filtered_features = [i for i in train.columns if(i not in targets and i not in drop_cols)]
num_features = [i for i in filtered_features if i not in cat_features]

print('cat_features :', len(cat_features), cat_features)
print('num_features :', len(num_features), num_features)
print('targets', targets)

#–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø—É—Ç–µ–º —É–º–Ω–æ–∂–µ–Ω–∏—è –∏ –¥–µ–ª–µ–Ω–∏—è –≤—Å–µ—Ö —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –ø–æ–∑–∂–µ —Å–¥–µ–ª–∞–µ–º —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é
for i in num_features:
  for j in num_features:
    if i != j:
      train[f'{i}/{j}'] = train[i]/train[j]
      train[f'{j}/{i}'] = train[j]/train[i]
      train[f'{i}*{j}'] = train[i]*train[j]
      train[f'{j}*{i}'] = train[j]*train[i]

for i in num_features:
  for j in num_features:
    if i != j:
      test[f'{i}/{j}'] = test[i]/test[j]
      test[f'{j}/{i}'] = test[j]/test[i]
      test[f'{i}*{j}'] = test[i]*test[j]
      test[f'{j}*{i}'] = test[j]*test[i]

drop_cols = ['car_id', 'target_reg', 'target_class']
targets = ['target_reg']
cat_features = ['car_type', 'fuel_type', 'model']

filtered_features = [i for i in train.columns if (i not in targets and i not in drop_cols)]
num_features = [i for i in filtered_features if i not in cat_features]

print('cat_features :', len(cat_features), cat_features)
print('num_features :', len(num_features), num_features)
print('targets', targets)

# —Ñ–∏–∫—Å–∏—Ä—É–µ–º —Ä–∞–Ω–¥–æ–º —Å—Ç–µ–π—Ç
RANDOM_STATE = 7575

"""## **FEATURE SELECTION**"""

X_train, X_val, y_train, y_val = train_test_split(train[filtered_features], train[targets], test_size=0.2, random_state=RANDOM_STATE)

train_dataset = Pool(X_train, y_train, cat_features=cat_features)
val_dataset = Pool(X_val, y_val, cat_features=cat_features)

# –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å catboost –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
cb_init_params = {
    'eval_metric': 'RMSE',
    'thread_count': -1,
    'task_type': 'CPU',
    'random_seed': RANDOM_STATE
}

model = CatBoostRegressor(**cb_init_params)
model.fit(train_dataset,
          eval_set=val_dataset,
          verbose=10,
          use_best_model=True)

# –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ catboost —Å–æ –≤—Å–µ–º–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
mean_squared_error(y_val, model.predict(X_val), squared=False)

# RMSE —Å–æ –≤—Å–µ–º–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –Ω–∞ –ª–∏–¥–µ—Ä–±–æ—Ä–¥–µ - 11.9
pd.DataFrame({'car_id': car_id, 'target_reg': model.predict(test[filtered_features]).reshape(-1)}).to_csv('all_features_base_cb.csv', index=False)

# –ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –ø–æ–º–æ—â—å—é get_feature_importance –∏ –æ—Ç–±–µ—Ä–µ–º –±–æ–ª—å—à–µ 0.8
fi = model.get_feature_importance(prettified=True)
fi[fi['Importances']>0.8]

# –ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –ø–æ–º–æ—â—å—é SHAP
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(val_dataset)
shap.summary_plot(shap_values, X_val)

"""–í–∏–¥–∏–º, —á—Ç–æ –ø—Ä–∏–∑–Ω–∞–∫–∏ 'mean_rating*speed max', 'rating/user uniq', 'speed max/deviation normal count', 'user uniq*speed max' –≤—ã–¥–µ–ª—è—é—Ç—Å—è, –∏ –º–æ–∂–Ω–æ –±—ã–ª–æ –±—ã –æ—Ç–æ–±—Ä–∞—Ç—å —Ç–æ–ª—å–∫–æ –∏—Ö, –Ω–æ —è –±—É–¥—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ get_feature_importance –±–æ–ª—å—à–µ 0.8, —Ç–∞–∫ –∫–∞–∫ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –Ω–µ –±–æ–ª—å—à–æ–π –∏ –º–æ–∂–Ω–æ –ø–æ–∏–≥—Ä–∞—Ç—å –∏ —Å –±–æ–ª–µ–µ –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""

# –Ω–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ —Å –Ω—É–∂–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
new_filtered_features = list(fi[fi['Importances']>0.8]['Feature Id'].values)

# –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ —Å–µ—Ç–∞ –∏ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ —Å —É–∂–µ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
train = train[new_filtered_features + targets]
test = test[new_filtered_features]

drop_cols = ['car_id', 'target_reg']
targets = ['target_reg']
cat_features = ['car_type', 'fuel_type', 'model']

filtered_features = [i for i in train.columns if (i not in targets and i not in drop_cols)]
num_features = [i for i in filtered_features if i not in cat_features]

print('cat_features :', len(cat_features), cat_features)
print('num_features :', len(num_features), num_features)
print('targets', targets)

# –ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é –ø—Ä–∏ –ø–æ–º–æ—â–∏ phik
phik_overview = train.phik_matrix().round(2).sort_values('target_reg')

plot_correlation_matrix(phik_overview.values,
                        x_labels=phik_overview.columns,
                        y_labels=phik_overview.index,
                        vmin=0, vmax=1, color_map="Greens",
                        title=r"correlation $\phi_K$",
                        fontsize_factor=0.8, figsize=(20, 10))
plt.tight_layout()

"""–ù–∞—Ö–æ–¥–∏–º –µ—â–µ –ø–∞—Ä—É –∫–æ–ª–æ–Ω–æ–∫, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –Ω–µ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—Ç —Å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π. –£–¥–∞–ª—è–µ–º –∏—Ö"""

train.drop(['rating_min*mean_rating', 'mean_rating*rating_min', 'rating_min/mean_rating', 'rating_min*year_to_start'], axis=1, inplace=True)
test.drop(['rating_min*mean_rating', 'mean_rating*rating_min', 'rating_min/mean_rating', 'rating_min*year_to_start'], axis=1, inplace=True)

drop_cols = ['car_id', 'target_reg']
targets = ['target_reg']
cat_features = ['car_type', 'fuel_type', 'model']

filtered_features = [i for i in train.columns if (i not in targets and i not in drop_cols)]
num_features = [i for i in filtered_features if i not in cat_features]

print('cat_features :', len(cat_features), cat_features)
print('num_features :', len(num_features), num_features)
print('targets', targets)

"""## **PIPELINES**

–°–æ–∑–¥–∞–µ–º –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–∏—Å–ª–æ–≤—ã—Ö –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
"""

numerical_pipeline = make_pipeline(
    StandardScaler()
)

categorical_pipeline = make_pipeline(
    OrdinalEncoder()
)

transformer = make_column_transformer(
    (
        numerical_pipeline,
        make_column_selector(dtype_include=['float'])
    ),
    (
        categorical_pipeline,
        make_column_selector(dtype_include=object)
    )
)

transformer

# —Å–ø–ª–∏—Ç –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
X = train[filtered_features]
y = train[targets]

# —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ —Ä–∞–Ω–µ–µ —Å–æ–∑–¥–∞–Ω–Ω–æ–º—É –ø–∞–π–ø–ª–∞–π–Ω—É
X = transformer.fit_transform(X)
X_test = transformer.fit_transform(test)

# –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º X –∏ X_test –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º—ã
X = pd.DataFrame(data=X, columns=transformer.get_feature_names_out(), index=train.index)
X_test = pd.DataFrame(data=X_test, columns=transformer.get_feature_names_out(), index=test.index)

"""## **FIT BASE MODELS**"""

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)

# —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
lst_base_models = [CatBoostRegressor, LGBMRegressor, XGBRegressor,
                   LinearRegression, KNeighborsRegressor, SVR,
                   RandomForestRegressor, DecisionTreeRegressor,
                   ]
preds = {}
base_models = {}

# –æ–±—É—á–∏–º –º–æ–¥–µ–ª–∏ –±–µ–∑ –ø–æ–¥–±–æ—Ä–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
for mod in tqdm(lst_base_models):

  if mod.__name__ == 'CatBoostRegressor':

    train_dataset = Pool(X_train, y_train)
    val_dataset = Pool(X_val, y_val)
    test_dataset = Pool(X_test)

    init_params = {
        'eval_metric': 'RMSE',
        'thread_count': -1,
        'task_type': 'CPU',
        'random_seed': RANDOM_STATE
    }
    model = mod(**init_params)
    model.fit(train_dataset,
              eval_set=val_dataset,
              verbose=False,
              use_best_model=True)
    preds.update({mod.__name__: mean_squared_error(y_val, model.predict(X_val), squared=False)})
    base_models.update({mod.__name__: model})

  if mod.__name__ == 'LGBMRegressor':

    train_dataset = Dataset(X_train, y_train)
    val_dataset = Dataset(X_val, y_val)
    test_dataset = Dataset(X_test)

    init_params = {
    'objective': 'regression',
    'random_state': RANDOM_STATE,
    'verbosity': -1,
    'device': 'cpu'
    }
    callbacks = [lgb.early_stopping(10, verbose=0), lgb.log_evaluation(period=0)]
    model = lgb.train(params=init_params,
                      train_set=train_dataset,
                      valid_sets=(val_dataset),
                      callbacks=callbacks)
    preds.update({mod.__name__: mean_squared_error(y_val, model.predict(X_val), squared=False)})
    base_models.update({mod.__name__: model})

  if mod.__name__ == 'XGBRegressor':

    train_dataset = xgb.DMatrix(X_train, label=y_train, nthread=-1)
    val_dataset = xgb.DMatrix(X_val, label=y_val, nthread=-1)
    test_dataset = xgb.DMatrix(X_test, nthread=-1)

    init_params = {
    'objective': 'reg:squarederror',
    'eval_metric': 'rmse',
    'random_state': RANDOM_STATE,
    'n_jobs': 2,
    'verbosity': 0
    }

    model = xgb.train(params=init_params,
                      dtrain=train_dataset,
                      evals=[(train_dataset, 'dtrain'), (val_dataset, 'dtest')],
                      verbose_eval=False,
                      early_stopping_rounds=25)
    preds.update({mod.__name__: mean_squared_error(y_val, model.predict(val_dataset), squared=False)})
    base_models.update({mod.__name__: model})

  else:
    model = mod().fit(X_train, y_train)
    preds.update({mod.__name__: mean_squared_error(y_val, model.predict(X_val), squared=False)})
    base_models.update({mod.__name__: model})

# —Ç–∞–±–ª–∏—Ü–∞ –º–æ–¥–µ–ª–µ–π –∏ –∏—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
pd.DataFrame(preds, index=['models'], columns=preds.keys())

"""RandomForestRegressor - –ª—É—á—à–∏–π –ø–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º, –Ω–æ –ø–æ—Å–º–æ—Ç—Ä–∏–º –¥–∞–ª—å—à–µ –∫–∞–∫ –±—É–¥—É—Ç –æ–±—Å—Ç–æ—è—Ç—å –¥–µ–ª–∞ –Ω–∞ –ª–∏–¥–µ—Ä–±–æ—Ä–¥–µ"""

lb = {}

for name, model in base_models.items():
  if name == 'XGBRegressor':
    test_dmatrix = xgb.DMatrix(X_test, nthread=-1)
    pred = model.predict(test_dmatrix).reshape(-1)
    pd.DataFrame({'car_id': car_id, 'target_reg': pred}).to_csv(f'base_{name}.csv', index=False)
    print(name)
    lb.update({name: input()})

  else:
    pred = model.predict(X_test).reshape(-1)
    pd.DataFrame({'car_id': car_id, 'target_reg': pred}).to_csv(f'base_{name}.csv', index=False)
    print(name)
    lb.update({name: input()})

# —Ç–∞–±–ª–∏—Ü–∞ –º–æ–¥–µ–ª–µ–π –∏ –∏—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –ª–∏–¥–µ—Ä–±–æ—Ä–¥–µ
pd.DataFrame(lb, index=['models'], columns=lb.keys())

"""–ù–∞ —ç—Ç–æ—Ç —Ä–∞–∑ –ª—É—á—à–∏–º –æ–∫–∞–∑–∞–ª—Å—è CatBoostRegressor, –Ω–µ–º–Ω–æ–≥–æ –æ—Ç –Ω–µ–≥–æ –æ—Ç—Å—Ç–∞–ª–∏ RandomForestRegressor, LGBMRegressor, XGBRegressor. –û—Å—Ç–∞–ª—å–Ω—ã–µ —Å–æ–≤—Å–µ–º –ø–ª–æ—Ö–æ —Å–µ–±—è –ø–æ–∫–∞–∑–∞–ª–∏

## **OPTUNA**

### CatBoostRegressor
"""

# —Å–æ–∑–¥–∞–Ω–∏–µ —Å–ø–∏—Å–∫–∞ estimators –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –±—É–¥—É—â–µ–º  –ø—Ä–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–º —Å—Ç–µ–∫–∏–Ω–≥–µ
estimators = []

def fit_catboost(trial, train, val):
    X_train, y_train = train
    X_val, y_val = val

    param = {
        'iterations' : 4000,
        "learning_rate": trial.suggest_float("learning_rate", 0.001, 0.05),
        "l2_leaf_reg": trial.suggest_float("l2_leaf_reg", 2, 50),
        "colsample_bylevel": trial.suggest_float("colsample_bylevel", 0.01, 0.95),
        "depth": trial.suggest_int("depth", 1, 10),
        "boosting_type": trial.suggest_categorical("boosting_type", ["Ordered", "Plain"]),
        "bootstrap_type": trial.suggest_categorical("bootstrap_type", ["Bayesian", "Bernoulli", "MVS"]),
        "used_ram_limit": "14gb",
        "eval_metric": "RMSE",
    }


    if param["bootstrap_type"] == "Bayesian":
        param["bagging_temperature"] = trial.suggest_float("bagging_temperature", 0, 20)

    elif param["bootstrap_type"] == "Bernoulli":
        param["subsample"] = trial.suggest_float("subsample", 0.1, 1)


    clf = CatBoostRegressor(
        **param,
        thread_count=-1,
        random_seed=RANDOM_STATE,
    )

    clf.fit(
        X_train,
        y_train,
        eval_set=(X_val, y_val),
        verbose=0,
        plot=False,
        early_stopping_rounds=50,
    )

    y_pred = clf.predict(X_val)
    return clf, y_pred

def objective(trial, return_models=False):
    n_splits = 10
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)
    X_train = X
    y_train = y

    scores, models = [], []

    for train_idx, valid_idx in kf.split(X_train):
        train_data = X_train.iloc[train_idx, :], y_train.iloc[train_idx]
        valid_data = X_train.iloc[valid_idx, :], y_train.iloc[valid_idx]

        # –ü–æ–¥–∞–µ–º trials –¥–ª—è –ø–µ—Ä–µ–±–æ—Ä–∞
        model, y_pred = fit_catboost(trial, train_data, valid_data)
        scores.append(mean_squared_error(y_pred, valid_data[1], squared=False))
        models.append(model)
        break


    result = np.mean(scores)

    if return_models:
        return result, models
    else:
        return result

study = optuna.create_study(direction="minimize")
study.optimize(objective,
               n_trials=100,
               n_jobs = -1,
               show_progress_bar=True,)

valid_scores, optuna_cb = objective(
    optuna.trial.FixedTrial(study.best_params),
    return_models=True,
)
valid_scores

# –¥–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–æ–∫ estimators –∫–æ—Ä—Ç–µ–∂ –∏–∑ –∏–º–µ–Ω–∏ –º–æ–¥–µ–ª–∏ –∏ —Å–∞–º—É –º–æ–¥–µ–ª—å —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
estimators.append((optuna_cb[0].__class__.__name__, optuna_cb[0]))

# RMSE=11.8
pd.DataFrame({'car_id': car_id, 'target_reg': optuna_cb[0].predict(X_test)}).to_csv(f'optuna_cb.csv', index=False)

"""### LGBMRegressor"""

def fit_lightgbm(trial, train, val):
    X_train, y_train = train
    X_val, y_val = val

    param = {
        'verbosity': -1,
        'n_estimators': trial.suggest_int('n_estimators', 32, 1024),
        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.5, log=True),
        'max_depth': trial.suggest_int('max_depth', 1, 10),
        'num_leaves': trial.suggest_int('num_leaves', 2, 1024),
        'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 10),
        'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),
        'subsample': trial.suggest_float('subsample', 0.001, 1),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1),
        'min_child_samples': trial.suggest_int('min_child_samples', 2, 1024),
        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
        'objective':trial.suggest_categorical('objective', ['rmse']),
        'metric': trial.suggest_categorical('metric', ['rmse']),
        'seed': RANDOM_STATE,
        'boosting_type': trial.suggest_categorical('boosting_type', ["gbdt", "dart", "goss"])
    }


    clf = LGBMRegressor(
        **param,
        thread_count=-1,
        random_seed=RANDOM_STATE,
    )

    clf.fit(
        X_train,
        y_train,
        eval_set=(X_val, y_val),
    )

    y_pred = clf.predict(X_val)
    return clf, y_pred

def objective(trial, return_models=False):
    n_splits = 10
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)
    X_train = X
    y_train = y

    scores, models = [], []

    for train_idx, valid_idx in kf.split(X_train):
        train_data = X_train.iloc[train_idx, :], y_train.iloc[train_idx]
        valid_data = X_train.iloc[valid_idx, :], y_train.iloc[valid_idx]

        # –ü–æ–¥–∞–µ–º trials –¥–ª—è –ø–µ—Ä–µ–±–æ—Ä–∞
        model, y_pred = fit_lightgbm(trial, train_data, valid_data)
        scores.append(mean_squared_error(y_pred, valid_data[1], squared=False))
        models.append(model)
        break


    result = np.mean(scores)

    if return_models:
        return result, models
    else:
        return result

study = optuna.create_study(direction="minimize")
study.optimize(objective,
               n_trials=100,
               n_jobs = -1,
               show_progress_bar=True,)

valid_scores, optuna_lgbm = objective(
    optuna.trial.FixedTrial(study.best_params),
    return_models=True,
)

# –¥–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–æ–∫ estimators –∫–æ—Ä—Ç–µ–∂ –∏–∑ –∏–º–µ–Ω–∏ –º–æ–¥–µ–ª–∏ –∏ —Å–∞–º—É –º–æ–¥–µ–ª—å —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
estimators.append((optuna_lgbm[0].__class__.__name__, optuna_lgbm[0]))

valid_scores

# RMSE=11.9
pd.DataFrame({'car_id': car_id, 'target_reg': optuna_lgbm[0].predict(X_test)}).to_csv(f'optuna_lgbm.csv', index=False)

"""### XGBRegressor"""

def fit_xgboost(trial, train, val):
    X_train, y_train = train
    X_val, y_val = val

    param = {
        'eta': trial.suggest_float('eta', 0.01, 0.3),
        "learning_rate": trial.suggest_float("learning_rate", 0.1, 0.5),
        'max_depth': trial.suggest_int('max_depth', 1, 10),
        'reg_lambda': trial.suggest_float('reg_lambda', 0.01, 10),
        'subsample': trial.suggest_float('subsample', 0.01, 1),
        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.01, 1),
        'random_state': RANDOM_STATE,
        "objective": "reg:squarederror",
        "eval_metric": 'rmse',
    }


    clf = XGBRegressor(
        **param,
        verbose_eval=0,
        #thread_count=-1,
        random_seed=RANDOM_STATE,
    )

    clf.fit(
        X_train,
        y_train,
        eval_set=[(X_train, y_train), (X_val, y_val)],
    )

    y_pred = clf.predict(X_val)
    return clf, y_pred

def objective(trial, return_models=False):
    n_splits = 10
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)
    X_train = X
    y_train = y

    scores, models = [], []

    for train_idx, valid_idx in kf.split(X_train):
        train_data = X_train.iloc[train_idx, :], y_train.iloc[train_idx]
        valid_data = X_train.iloc[valid_idx, :], y_train.iloc[valid_idx]

        # –ü–æ–¥–∞–µ–º trials –¥–ª—è –ø–µ—Ä–µ–±–æ—Ä–∞
        model, y_pred = fit_xgboost(trial, train_data, valid_data)
        scores.append(mean_squared_error(y_pred, valid_data[1], squared=False))
        models.append(model)
        break


    result = np.mean(scores)

    if return_models:
        return result, models
    else:
        return result

study = optuna.create_study(direction="minimize")
study.optimize(objective,
               n_trials=100,
               #n_jobs = -1,
               show_progress_bar=True,)

valid_scores, optuna_xgb = objective(
    optuna.trial.FixedTrial(study.best_params),
    return_models=True,
)

# –¥–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–æ–∫ estimators –∫–æ—Ä—Ç–µ–∂ –∏–∑ –∏–º–µ–Ω–∏ –º–æ–¥–µ–ª–∏ –∏ —Å–∞–º—É –º–æ–¥–µ–ª—å —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
estimators.append((optuna_xgb[0].__class__.__name__, optuna_xgb[0]))

valid_scores

# RMSE=11.8
pd.DataFrame({'car_id': car_id, 'target_reg': optuna_xgb[0].predict(X_test)}).to_csv(f'optuna_xgb.csv', index=False)

"""### LinearRegression"""

# –Ω–µ—Ç –ø–æ–¥–±–æ—Ä–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –Ω–æ –º–æ–∂–µ–º –æ–±—É—á–∏—Ç—å –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ–ª–¥–∞—Ö
def objective_lr(X, y):
  X, y = X, y
  n_splits = 10
  kf = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)

  scores, models = [], []

  for train_idx, valid_idx in kf.split(X):
    X_train, y_train = X.iloc[train_idx, :], y.iloc[train_idx]
    X_val, y_val = X.iloc[valid_idx, :], y.iloc[valid_idx]

    clf = LinearRegression()
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_val)

    scores.append(mean_squared_error(y_pred, y_val, squared=False))
    models.append(clf)
    break


  result = np.mean(scores)
  return result, models

valid_scores, optuna_lr = objective_lr(X, y)
valid_scores

# –¥–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–æ–∫ estimators –∫–æ—Ä—Ç–µ–∂ –∏–∑ –∏–º–µ–Ω–∏ –º–æ–¥–µ–ª–∏ –∏ —Å–∞–º—É –º–æ–¥–µ–ª—å —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
estimators.append((optuna_lr[0].__class__.__name__, optuna_lr[0]))

# RMSE=19.5
pd.DataFrame({'car_id': car_id, 'target_reg': optuna_lr[0].predict(X_test).reshape(-1)}).to_csv(f'optuna_lr.csv', index=False)

"""### KNeighborsRegressor"""

def fit_knn(trial, train, val):
    X_train, y_train = train
    X_val, y_val = val

    param = {
        'n_neighbors': trial.suggest_int('n_neighbors', 2, 10),
        'weights': trial.suggest_categorical('weights', ['uniform', 'distance']),
        "leaf_size": trial.suggest_int("leaf_size", 1, 20),
        'p': trial.suggest_int('p', 1, 2),
        "algorithm": trial.suggest_categorical("algorithm", ['auto', 'ball_tree', 'kd_tree', 'brute']),
        'metric': trial.suggest_categorical('metric', ['euclidean','manhattan'])
    }

    clf = KNeighborsRegressor(**param)

    clf.fit(X_train, y_train)

    y_pred = clf.predict(X_val)
    return clf, y_pred

def objective(trial, return_models=False):
    n_splits = 10
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)
    X_train = X
    y_train = y

    scores, models = [], []

    for train_idx, valid_idx in kf.split(X_train):
        train_data = X_train.iloc[train_idx, :], y_train.iloc[train_idx]
        valid_data = X_train.iloc[valid_idx, :], y_train.iloc[valid_idx]

        # –ü–æ–¥–∞–µ–º trials –¥–ª—è –ø–µ—Ä–µ–±–æ—Ä–∞
        model, y_pred = fit_knn(trial, train_data, valid_data)
        scores.append(mean_squared_error(y_pred, valid_data[1], squared=False))
        models.append(model)
        break


    result = np.mean(scores)

    if return_models:
        return result, models
    else:
        return result

study = optuna.create_study(direction="minimize")
study.optimize(objective,
               n_trials=100,
               n_jobs = -1,
               show_progress_bar=True,)

valid_scores, optuna_knn = objective(
    optuna.trial.FixedTrial(study.best_params),
    return_models=True,
)

# –¥–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–æ–∫ estimators –∫–æ—Ä—Ç–µ–∂ –∏–∑ –∏–º–µ–Ω–∏ –º–æ–¥–µ–ª–∏ –∏ —Å–∞–º—É –º–æ–¥–µ–ª—å —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
estimators.append((optuna_knn[0].__class__.__name__, optuna_knn[0]))

valid_scores

# RMSE=12.7
pd.DataFrame({'car_id': car_id, 'target_reg': optuna_knn[0].predict(X_test).reshape(-1)}).to_csv(f'optuna_knn.csv', index=False)

"""### SVR"""

def fit_svr(trial, train, val):
    X_train, y_train = train
    X_val, y_val = val

    param = {
        'gamma': trial.suggest_categorical('gamma', ['scale', 'auto']),
        'kernel': trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf', 'sigmoid']),
        'tol': trial.suggest_float('tol', 1e-4, 1e-2),
        'C': trial.suggest_float('C', 0.1, 9),
        'epsilon': trial.suggest_float('epsilon', 0.01, 0.2),
    }

    clf = SVR(**param)

    clf.fit(X_train, y_train)

    y_pred = clf.predict(X_val)
    return clf, y_pred

def objective(trial, return_models=False):
    n_splits = 10
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)
    X_train = X
    y_train = y

    scores, models = [], []

    for train_idx, valid_idx in kf.split(X_train):
        train_data = X_train.iloc[train_idx, :], y_train.iloc[train_idx]
        valid_data = X_train.iloc[valid_idx, :], y_train.iloc[valid_idx]

        # –ü–æ–¥–∞–µ–º trials –¥–ª—è –ø–µ—Ä–µ–±–æ—Ä–∞
        model, y_pred = fit_svr(trial, train_data, valid_data)
        scores.append(mean_squared_error(y_pred, valid_data[1], squared=False))
        models.append(model)
        break


    result = np.mean(scores)

    if return_models:
        return result, models
    else:
        return result

study = optuna.create_study(direction="minimize")
study.optimize(objective,
               n_trials=100,
               n_jobs = -1,
               show_progress_bar=True,)

valid_scores, optuna_svr = objective(
    optuna.trial.FixedTrial(study.best_params),
    return_models=True,
)

# –¥–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–æ–∫ estimators –∫–æ—Ä—Ç–µ–∂ –∏–∑ –∏–º–µ–Ω–∏ –º–æ–¥–µ–ª–∏ –∏ —Å–∞–º—É –º–æ–¥–µ–ª—å —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
estimators.append((optuna_svr[0].__class__.__name__, optuna_svr[0]))

valid_scores

# RMSE=12.6
pd.DataFrame({'car_id': car_id, 'target_reg': optuna_svr[0].predict(X_test).reshape(-1)}).to_csv(f'optuna_svr.csv', index=False)

"""### RandomForestRegressor"""

def fit_rfr(trial, train, val):
    X_train, y_train = train
    X_val, y_val = val

    param = {
        'max_depth': trial.suggest_int('max_depth', 4, 10),
        'max_features': trial.suggest_categorical("max_features", ["auto", 'sqrt', 'log2']),
        'n_estimators': trial.suggest_int('n_estimators', 5, 400),
        'criterion': trial.suggest_categorical('criterion', ['squared_error']),
        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),
        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 2, 10),
        'random_state': RANDOM_STATE
    }

    clf = RandomForestRegressor(**param, n_jobs=-1)

    clf.fit(X_train, y_train)

    y_pred = clf.predict(X_val)
    return clf, y_pred

def objective(trial, return_models=False):
    n_splits = 10
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)
    X_train = X
    y_train = y

    scores, models = [], []

    for train_idx, valid_idx in kf.split(X_train):
        train_data = X_train.iloc[train_idx, :], y_train.iloc[train_idx]
        valid_data = X_train.iloc[valid_idx, :], y_train.iloc[valid_idx]

        # –ü–æ–¥–∞–µ–º trials –¥–ª—è –ø–µ—Ä–µ–±–æ—Ä–∞
        model, y_pred = fit_rfr(trial, train_data, valid_data)
        scores.append(mean_squared_error(y_pred, valid_data[1], squared=False))
        models.append(model)
        break


    result = np.mean(scores)

    if return_models:
        return result, models
    else:
        return result

study = optuna.create_study(direction="minimize")
study.optimize(objective,
               n_trials=100,
               n_jobs = -1,
               show_progress_bar=True,)

valid_scores, optuna_rfr = objective(
    optuna.trial.FixedTrial(study.best_params),
    return_models=True,
)

# –¥–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–æ–∫ estimators –∫–æ—Ä—Ç–µ–∂ –∏–∑ –∏–º–µ–Ω–∏ –º–æ–¥–µ–ª–∏ –∏ —Å–∞–º—É –º–æ–¥–µ–ª—å —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
estimators.append((optuna_rfr[0].__class__.__name__, optuna_rfr[0]))

valid_scores

# RMSE=12.2
pd.DataFrame({'car_id': car_id, 'target_reg': optuna_rfr[0].predict(X_test)}).to_csv(f'optuna_rfr.csv', index=False)

"""### DecisionTreeRegressor"""

def fit_tree(trial, train, val):
    X_train, y_train = train
    X_val, y_val = val

    param = {"max_depth": trial.suggest_int("max_depth", 2, 10),
             "min_samples_split": trial.suggest_int("min_samples_split", 2, 10),
             "min_samples_leaf": trial.suggest_int("min_samples_leaf", 2, 8),
             "splitter": trial.suggest_categorical('splitter', ['best','random']),
             "max_features": trial.suggest_categorical("max_features", ['auto', 'sqrt', 'log2']),
             'criterion': trial.suggest_categorical('criterion', ['squared_error']),
             'random_state': RANDOM_STATE
             }

    clf = DecisionTreeRegressor(**param)

    clf.fit(X_train, y_train)

    y_pred = clf.predict(X_val)
    return clf, y_pred

def objective(trial, return_models=False):
    n_splits = 10
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)
    X_train = X
    y_train = y

    scores, models = [], []

    for train_idx, valid_idx in kf.split(X_train):
        train_data = X_train.iloc[train_idx, :], y_train.iloc[train_idx]
        valid_data = X_train.iloc[valid_idx, :], y_train.iloc[valid_idx]

        # –ü–æ–¥–∞–µ–º trials –¥–ª—è –ø–µ—Ä–µ–±–æ—Ä–∞
        model, y_pred = fit_tree(trial, train_data, valid_data)
        scores.append(mean_squared_error(y_pred, valid_data[1], squared=False))
        models.append(model)
        break


    result = np.mean(scores)

    if return_models:
        return result, models
    else:
        return result

study = optuna.create_study(direction="minimize")
study.optimize(objective,
               n_trials=100,
               n_jobs = -1,
               show_progress_bar=True,)

valid_scores, optuna_tree = objective(
    optuna.trial.FixedTrial(study.best_params),
    return_models=True,
)

# –¥–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–æ–∫ estimators –∫–æ—Ä—Ç–µ–∂ –∏–∑ –∏–º–µ–Ω–∏ –º–æ–¥–µ–ª–∏ –∏ —Å–∞–º—É –º–æ–¥–µ–ª—å —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
estimators.append((optuna_tree[0].__class__.__name__, optuna_tree[0]))

valid_scores

# RMSE=12.4
pd.DataFrame({'car_id': car_id, 'target_reg': optuna_tree[0].predict(X_test)}).to_csv(f'optuna_tree.csv', index=False)

"""## BLENDING

### Mean, Median
"""

# –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –¥–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏ –∫–∞–∫ –æ—Ç—Ä–∞–±–æ—Ç–∞–ª–∏ –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –∏ –Ω–∞ –ª–∏–¥–µ—Ä–±–æ—Ä–¥–µ
scores = pd.DataFrame()

scores['models'] = ['CatBoostRegressor','LGBMRegressor','XGBRegressor','LinearRegression',
                    'KNeighborsRegressor','SVR','RandomForestRegressor', 'DecisionTreeRegressor']

scores['validation'] = [10.4, 10.5, 10.6, 11.4, 11.7, 10.8, 10.5, 10.8]
scores['leaderboard'] = [11.8, 11.9, 11.8, 19.5, 12.7, 12.6, 12.2, 12.4]
scores.sort_values('leaderboard')

# –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º —Å–æ –≤—Å–µ–º–∏ –º–æ–¥–µ–ª—è–º–∏ –∏ –∏—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏ –¥–ª—è —É–¥–æ–±–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø—Ä–∏ –±–ª–µ–Ω–¥–∏–Ω–≥–µ
preds_models = pd.DataFrame(
{'CatBoostRegressor': pd.read_csv('optuna_cb.csv')['target_reg'],
'LGBMRegressor': pd.read_csv('optuna_lgbm.csv')['target_reg'],
'XGBRegressor': pd.read_csv('optuna_xgb.csv')['target_reg'],
'LinearRegression': pd.read_csv('optuna_lr.csv')['target_reg'],
'KNeighborsRegressor': pd.read_csv('optuna_knn.csv')['target_reg'],
'SVR': pd.read_csv('optuna_svr.csv')['target_reg'],
'RandomForestRegressor': pd.read_csv('optuna_rfr.csv')['target_reg'],
'DecisionTreeRegressor': pd.read_csv('optuna_tree.csv')['target_reg']
}
)
preds_models

cols_for_blending = ['CatBoostRegressor',
                     'LGBMRegressor',
                     'XGBRegressor',
                     'LinearRegression',
                    'KNeighborsRegressor',
                     'SVR',
                     'RandomForestRegressor',
                     'DecisionTreeRegressor']

# –±–ª–µ–Ω–¥–∏–Ω–≥ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –ø–æ —Å—Ä–µ–¥–Ω–µ–º—É –∑–Ω–∞—á–µ–Ω–∏—é –∏ –º–µ–¥–∏–∞–Ω–µ
preds_models['all_mod_mean'] = preds_models[cols_for_blending].mean(axis=1)
preds_models['all_mod_median'] = preds_models[cols_for_blending].median(axis=1)

# RMSE=12.1
pd.DataFrame({'car_id': car_id, 'target_reg': preds_models['all_mod_mean']}).to_csv(f'blend_all_mod_mean.csv', index=False)

# RMSE=11.8
pd.DataFrame({'car_id': car_id, 'target_reg': preds_models['all_mod_median']}).to_csv(f'blend_all_mod_median.csv', index=False)

cols_for_blending = ['CatBoostRegressor',
                     #'LGBMRegressor',
                     'XGBRegressor',
                     #'LinearRegression',
                    #'KNeighborsRegressor',
                     #'SVR',
                     'RandomForestRegressor',
                     #'DecisionTreeRegressor'
                     ]

# –±–ª–µ–Ω–¥–∏–Ω–≥ 2-–º–æ–¥–µ–ª–µ–π –±—É—Å—Ç–∏–Ω–≥–∞ –∏ 1-–æ–π –º–æ–¥–µ–ª–∏ —Å–ª—É—á–∞–π–Ω–æ–≥–æ –ª–µ—Å–∞
preds_models['rf_cb_xgb_mean'] = preds_models[cols_for_blending].mean(axis=1)
preds_models['rf_cb_xgb_median'] = preds_models[cols_for_blending].median(axis=1)

# RMSE=11.8
pd.DataFrame({'car_id': car_id, 'target_reg': preds_models['rf_cb_xgb_mean']}).to_csv(f'rf_cb_xgb_mean.csv', index=False)

# RMSE=11.8
pd.DataFrame({'car_id': car_id, 'target_reg': preds_models['rf_cb_xgb_median']}).to_csv(f'rf_cb_xgb_median.csv', index=False)

"""### Weight"""

# –¥–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏, —á—Ç–æ–±—ã –ø–æ–Ω–∏–º–∞—Ç—å –Ω–∞ –∫–∞–∫–∏–µ –º–æ–¥–µ–ª–∏ –¥–µ–ª–∞—Ç—å —Å—Ç–∞–≤–∫–∏ –ø—Ä–∏ –ø–æ–¥–±–æ—Ä–µ –≤–µ—Å–æ–≤
scores[['models', 'leaderboard']].sort_values('leaderboard')

# 3 –º–æ–¥–µ–ª–∏ –±—É—Å—Ç–∏–Ω–≥–∞
preds_models['weight_cb_lgbm_xgb'] = preds_models['CatBoostRegressor'] * 0.45 + preds_models['LGBMRegressor'] * 0.1 + preds_models['XGBRegressor'] * 0.45

# RMSE=11.7
pd.DataFrame({'car_id': car_id, 'target_reg': preds_models['weight_cb_lgbm_xgb']}).to_csv(f'weight_cb_lgbm_xgb.csv', index=False)

"""–° –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π –≤–µ—Å–æ–≤ –ª—É—á—à–∏—Ö –º–æ–¥–µ–ª–µ–π(–≤—Å–µ –±—É—Å—Ç–∏–Ω–≥–∏) —É–¥–∞–ª–æ—Å—å –µ—â–µ –Ω–µ–º–Ω–æ–≥–æ —É–ª—É—á—à–∏—Ç—å —Å–∫–æ—Ä"""

# 2 –º–æ–¥–µ–ª–∏ –±—É—Å—Ç–∏–Ω–≥–∞ –∏ 1 –º–æ–¥–µ–ª—å —Å–ª—É—á–∞–π–Ω–æ–≥–æ –ª–µ—Å–∞
preds_models['weight_rf_cb_xgb'] = preds_models['CatBoostRegressor'] * 0.45 + preds_models['XGBRegressor'] * 0.45 + preds_models['RandomForestRegressor'] * 0.1

# RMSE=11.7
pd.DataFrame({'car_id': car_id, 'target_reg': preds_models['weight_rf_cb_xgb']}).to_csv(f'weight_rf_cb_xgb.csv', index=False)

"""–° –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π –≤–µ—Å–æ–≤ 2-—Ö –º–æ–¥–µ–ª–µ–π –±—É—Å—Ç–∏–Ω–≥–∞ –∏ 1-–æ–π –º–æ–¥–µ–ª–∏ —Å–ª—É—á–∞–π–Ω–æ–≥–æ –ª–µ—Å–∞ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ª—É—á—à–∏–π —Å–∫–æ—Ä, –ø—Ä–∏—Ö–æ–∂—É –∫ –≤—ã–≤–æ–¥—É, —á—Ç–æ catboost –∏ xgboost —Ç–∞—â–∞—Ç, —Ç–∞–∫ –∫–∞–∫ —Å–ª—É—á–∞–π–Ω–æ–º—É –ª–µ—Å—É –¥–æ–±–∞–≤–∏–ª –º–∞–ª–µ–Ω—å–∫–∏–π –≤–µ—Å. –•–æ—Ç—è –ø–æ —Ñ–∞–∫—Ç—É –±—ã–ª–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ –º–Ω–æ–≥–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –≤–µ—Å–æ–≤, —É–ª—É—á—à–∏—Ç—å —Å–∫–æ—Ä 11.7 –Ω–µ —É–¥–∞–ª–æ—Å—å

## STACKING

### StackingRegressor
"""

# –≤ –∫–∞—á–µ—Å—Ç–≤–µ –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏ –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å RandomForestRegressor
meta_model = StackingRegressor(
    estimators=estimators,
    #estimators=estimators[1:3]+estimators[5:7],
    # estimators=[estimators[1], estimators[2], estimators[6]],
    #final_estimator=LinearRegression(),
    final_estimator=RandomForestRegressor(n_estimators = 10_000,
                                            max_depth = 5,
                                            verbose=False),
    n_jobs=-1,
    verbose=False,
)

stacking_reg = meta_model
stacking_reg

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)

stacking_reg.fit(X_train, y_train)

# –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
mean_squared_error(y_val, stacking_reg.predict(X_val), squared=False)

# RMSE=12.0
pd.DataFrame({'car_id': car_id, 'target_reg': stacking_reg.predict(X_test)}).to_csv(f'stacking_reg_rf.csv', index=False)

all_rmse = {}
corr_df = pd.DataFrame()

for model, (name, _) in zip(stacking_reg.estimators_, stacking_reg.estimators):
    preprocessed = stacking_reg.estimators[0][1].fit(X_train, y_train)
    all_rmse.update({name: mean_squared_error(model.predict(X_val), y_val, squared=False)})

    corr_df[name] = model.predict(X_val)

sorted(all_rmse.items(), key=lambda item: item[1])

corr_df.corr().style.background_gradient(cmap="RdYlGn")

"""–ú–µ–Ω–µ–µ —Å–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—Ç LGBMRegressor, SVR, RandomForest, XGB, –≤–ø—Ä–∏–Ω—Ü–∏–ø–µ –∏ –ø–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º —ç—Ç–æ –ª—É—á—à–µ –≤–∏–¥–Ω–æ, –æ—Å—Ç–∞–≤–ª—è—é –≥—Ä–∞—Ñ–∏–∫ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π, —Ç–∞–∫ –∫–∞–∫ –ª—É—á—à–∏–π —Å–∫–æ—Ä –±—ã–ª –¥–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏—à—å 11.9 –¥–∞–∂–µ –ø—Ä–∏ —É–±—Ä–∞–Ω–Ω—ã—Ö —Å–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π"""

preds_models['stack_lr_lgbm_rf_xgb'] = stacking_reg.predict(X_test)

# RMSE=11.9
pd.DataFrame({'car_id': car_id, 'target_reg': preds_models['stack_lr_lgbm_rf_xgb']}).to_csv(f'stack_lr_lgbm_rf_xgb.csv', index=False)

"""### Stacking without extra features"""

# –í–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è —Ñ—É–Ω–∫—Ü–∏–µ–π –∏–∑ —É—á–µ–±–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞ –ø—Ä–æ —Å—Ç–µ–∫–∏–Ω–≥
def GetPreds(model, X, y, X_test, n_fold=5):

    folds = KFold(n_splits=n_fold)
    preds = np.empty((0,1), float)
    df_y = pd.DataFrame(data=y)

    for train_indices, val_indices in folds.split(X, y):
        model.fit(X.iloc[train_indices, :], df_y.iloc[train_indices])
        predictions = model.predict(X.iloc[val_indices, :])
        preds = np.concatenate((preds, predictions), axis=None)

    model.fit(X, y)
    test_preds = model.predict(X_test)
    return preds.reshape(len(X), 1), test_preds.reshape(len(X_test), 1)

# –≤—ã–±–∏—Ä–∞–µ–º 5 –º–æ–¥–µ–ª–µ–π —Ö–æ—Ä–æ—à–æ —Å–µ–±—è –ø–æ–∫–∞–∑–∞–≤—à–∏—Ö –ø—Ä–∏ StackingRegression
auto_stacking_models = estimators[:3]+estimators[5:7]

# —Å–æ–∑–¥–∞–¥–∏–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º—ã –¥–ª—è —Å–±–æ—Ä–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π (–≤–∞–ª–∏–¥–∞—Ü–∏—è –∏ —Ç–µ—Å—Ç)
train_stack_df = y.copy()
test_stack_df = X_test[['pipeline-1__mean_rating*speed_max']].copy()

# —Å–æ–∑–¥–∞–µ–º —Ü–∏–∫–ª –≤ –∫–æ—Ç–æ—Ä–æ–º –ø—Ä–æ–π–¥–µ–º—Å—è –ø–æ —Å–ø–∏—Å–∫—É 5 —É—Å–ø–µ—à–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –¥–æ–±–∞–≤–∏–º –≤–æ
# –≤–Ω–æ–≤—å —Å–æ–∑–¥–∞–Ω–Ω—ã–µ –≤—ã—à–µ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º—ã –¥–ª—è —Å–±–æ—Ä–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
for mod in auto_stacking_models:
  train_preds, test_preds = GetPreds(model=mod[1], X=X, y=y, X_test=X_test)
  name = mod[1].__class__.__name__
  train_stack_df[name] = train_preds
  test_stack_df[name] = test_preds

# —É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞—Ö
train_stack_df = train_stack_df.iloc[:, 1:]
test_stack_df = test_stack_df.iloc[:, 1:]

"""#### Staking CatboostRegressor"""

def fit_st_no_feat_cb(trial, train, val):
    X_train, y_train = train
    X_val, y_val = val

    param = {
        'iterations' : 4000,
        "learning_rate": trial.suggest_float("learning_rate", 0.001, 0.05),
        "l2_leaf_reg": trial.suggest_float("l2_leaf_reg", 2, 50),
        "colsample_bylevel": trial.suggest_float("colsample_bylevel", 0.01, 0.95),
        "depth": trial.suggest_int("depth", 1, 10),
        "boosting_type": trial.suggest_categorical("boosting_type", ["Ordered", "Plain"]),
        "bootstrap_type": trial.suggest_categorical("bootstrap_type", ["Bayesian", "Bernoulli", "MVS"]),
        "used_ram_limit": "14gb",
        "eval_metric": "RMSE",
    }


    if param["bootstrap_type"] == "Bayesian":
        param["bagging_temperature"] = trial.suggest_float("bagging_temperature", 0, 20)

    elif param["bootstrap_type"] == "Bernoulli":
        param["subsample"] = trial.suggest_float("subsample", 0.1, 1)


    clf = CatBoostRegressor(
        **param,
        thread_count=-1,
        random_seed=RANDOM_STATE,
    )

    clf.fit(
        X_train,
        y_train,
        eval_set=(X_val, y_val),
        verbose=0,
        plot=False,
        early_stopping_rounds=50,
    )

    y_pred = clf.predict(X_val)
    return clf, y_pred

def objective(trial, return_models=False):
    n_splits = 10
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)
    X_train = train_stack_df
    y_train = y

    scores, models = [], []

    for train_idx, valid_idx in kf.split(X_train):
        train_data = X_train.iloc[train_idx, :], y_train.iloc[train_idx]
        valid_data = X_train.iloc[valid_idx, :], y_train.iloc[valid_idx]

        # –ü–æ–¥–∞–µ–º trials –¥–ª—è –ø–µ—Ä–µ–±–æ—Ä–∞
        model, y_pred = fit_st_no_feat_cb(trial, train_data, valid_data)
        scores.append(mean_squared_error(y_pred, valid_data[1], squared=False))
        models.append(model)
        break


    result = np.mean(scores)

    if return_models:
        return result, models
    else:
        return result

study = optuna.create_study(direction="minimize")
study.optimize(objective,
               n_trials=100,
               n_jobs = -1,
               show_progress_bar=True,)

valid_scores, optuna_st_no_feat_cb = objective(
    optuna.trial.FixedTrial(study.best_params),
    return_models=True,
)

valid_scores

# RMSE=11.8
pd.DataFrame({'car_id': car_id, 'target_reg': optuna_st_no_feat_cb[0].predict(test_stack_df).reshape(-1)}).to_csv(f'optuna_st_no_feat_cb.csv', index=False)

"""#### Stacking DecisionTreeRegressor"""

def fit_st_no_feat_tree(trial, train, val):
    X_train, y_train = train
    X_val, y_val = val

    param = {"max_depth": trial.suggest_int("max_depth", 2, 10),
             "min_samples_split": trial.suggest_int("min_samples_split", 2, 10),
             "min_samples_leaf": trial.suggest_int("min_samples_leaf", 2, 8),
             "splitter": trial.suggest_categorical('splitter', ['best','random']),
             "max_features": trial.suggest_categorical("max_features", ['auto', 'sqrt', 'log2']),
             'criterion': trial.suggest_categorical('criterion', ['squared_error']),
             'random_state': RANDOM_STATE
             }

    clf = DecisionTreeRegressor(**param)

    clf.fit(X_train, y_train)

    y_pred = clf.predict(X_val)
    return clf, y_pred

def objective(trial, return_models=False):
    n_splits = 10
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)
    X_train = train_stack_df
    y_train = y

    scores, models = [], []

    for train_idx, valid_idx in kf.split(X_train):
        train_data = X_train.iloc[train_idx, :], y_train.iloc[train_idx]
        valid_data = X_train.iloc[valid_idx, :], y_train.iloc[valid_idx]

        # –ü–æ–¥–∞–µ–º trials –¥–ª—è –ø–µ—Ä–µ–±–æ—Ä–∞
        model, y_pred = fit_st_no_feat_tree(trial, train_data, valid_data)
        scores.append(mean_squared_error(y_pred, valid_data[1], squared=False))
        models.append(model)
        break


    result = np.mean(scores)

    if return_models:
        return result, models
    else:
        return result

study = optuna.create_study(direction="minimize")
study.optimize(objective,
               n_trials=100,
               n_jobs = -1,
               show_progress_bar=True,)

valid_scores, optuna_st_no_feat_tree = objective(
    optuna.trial.FixedTrial(study.best_params),
    return_models=True,
)

valid_scores

# RMSE=12.0
pd.DataFrame({'car_id': car_id, 'target_reg': optuna_st_no_feat_tree[0].predict(test_stack_df).reshape(-1)}).to_csv(f'optuna_st_no_feat_tree.csv', index=False)

"""#### Staking RandomForestRegressor"""

def fit_st_no_feat_rfr(trial, train, val):
    X_train, y_train = train
    X_val, y_val = val

    param = {
        'max_depth': trial.suggest_int('max_depth', 4, 10),
        'max_features': trial.suggest_categorical("max_features", ["auto", 'sqrt', 'log2']),
        'n_estimators': trial.suggest_int('n_estimators', 5, 400),
        'criterion': trial.suggest_categorical('criterion', ['squared_error']),
        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),
        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 2, 10),
        'random_state': RANDOM_STATE
    }

    clf = RandomForestRegressor(**param, n_jobs=-1)

    clf.fit(X_train, y_train)

    y_pred = clf.predict(X_val)
    return clf, y_pred

def objective(trial, return_models=False):
    n_splits = 10
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)
    X_train = train_stack_df
    y_train = y

    scores, models = [], []

    for train_idx, valid_idx in kf.split(X_train):
        train_data = X_train.iloc[train_idx, :], y_train.iloc[train_idx]
        valid_data = X_train.iloc[valid_idx, :], y_train.iloc[valid_idx]

        # –ü–æ–¥–∞–µ–º trials –¥–ª—è –ø–µ—Ä–µ–±–æ—Ä–∞
        model, y_pred = fit_st_no_feat_rfr(trial, train_data, valid_data)
        scores.append(mean_squared_error(y_pred, valid_data[1], squared=False))
        models.append(model)
        break


    result = np.mean(scores)

    if return_models:
        return result, models
    else:
        return result

study = optuna.create_study(direction="minimize")
study.optimize(objective,
               n_trials=100,
               n_jobs = -1,
               show_progress_bar=True,)

valid_scores, optuna_st_no_feat_rfr = objective(
    optuna.trial.FixedTrial(study.best_params),
    return_models=True,
)

valid_scores

# RMSE=12.0
pd.DataFrame({'car_id': car_id, 'target_reg': optuna_st_no_feat_rfr[0].predict(test_stack_df)}).to_csv(f'optuna_st_no_feat_rfr.csv', index=False)

"""### Staking with extra features

Feature engineering –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
"""

# –¥–æ–±–∞–≤–∏–º –º–µ–¥–∏–∞–Ω—É –∏ –º–æ–¥—É, —Ç–∞–∫ –∫–∞–∫ –ø—Ä–∏ –±–ª–µ–Ω–¥–∏–Ω–≥–µ –æ–Ω–∏ –Ω–µ–ø–ª–æ—Ö–æ —Å–µ–±—è –ø–æ–∫–∞–∑–∞–ª–∏
train_stack_df['median'] = train_stack_df.median(axis=1)
train_stack_df['mode'] = train_stack_df.mode(axis=1)

test_stack_df['median'] = test_stack_df.median(axis=1)
test_stack_df['mode'] = test_stack_df.mode(axis=1)

# –¥–æ–±–∞–≤–∏–º —É–º–Ω–æ–∂–µ–Ω–∏–µ –∏ –¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
for i in train_stack_df:
  for j in train_stack_df:
    if i != j:
      train_stack_df[f'{i}_div_{j}'] = train_stack_df[i]/train_stack_df[j]
      train_stack_df[f'{i}_mul_{j}'] = train_stack_df[i]*train_stack_df[j]

      test_stack_df[f'{i}_div_{j}'] = test_stack_df[i]/test_stack_df[j]
      test_stack_df[f'{i}_mul_{j}'] = test_stack_df[i]*test_stack_df[j]

"""–ü—Ä–æ–≤–µ–¥–µ–º —Å–Ω–æ–≤–∞ –ø—Ä–∏ –ø–æ–º–æ—â–∏ catboost feature selection —Ç–∞–∫ –∫–∞–∫ –ø–æ—Å–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–æ–≤—ã—Ö —Ñ–∏—á–µ–π, –∏—Ö —Å—Ç–∞–ª–æ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ"""

X_tr, X_v, y_tr, y_v = train_test_split(train_stack_df, y, test_size=0.2, random_state=RANDOM_STATE)

tr_dataset = Pool(X_tr, y_tr)
v_dataset = Pool(X_v, y_v)

# –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å catboost –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
cb_init_params_new = {
    'eval_metric': 'RMSE',
    'thread_count': -1,
    'task_type': 'CPU',
    'random_seed': RANDOM_STATE
}

model = CatBoostRegressor(**cb_init_params_new)
model.fit(tr_dataset,
          eval_set=v_dataset,
          verbose=10,
          use_best_model=True)

# –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
fi = model.get_feature_importance(prettified=True)

# –≤–æ–∑—å–º–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –±–æ–ª—å—à–µ 1
cols_importance = list(fi[fi['Importances']>1]['Feature Id'].values)

cols_importance

"""#### Stacking CatboostRegressor"""

def fit_st_with_feat_cb(trial, train, val):
    X_train, y_train = train
    X_val, y_val = val

    param = {
        'iterations' : 4000,
        "learning_rate": trial.suggest_float("learning_rate", 0.001, 0.05),
        "l2_leaf_reg": trial.suggest_float("l2_leaf_reg", 2, 50),
        "colsample_bylevel": trial.suggest_float("colsample_bylevel", 0.01, 0.95),
        "depth": trial.suggest_int("depth", 1, 10),
        "boosting_type": trial.suggest_categorical("boosting_type", ["Ordered", "Plain"]),
        "bootstrap_type": trial.suggest_categorical("bootstrap_type", ["Bayesian", "Bernoulli", "MVS"]),
        "used_ram_limit": "14gb",
        "eval_metric": "RMSE",
    }


    if param["bootstrap_type"] == "Bayesian":
        param["bagging_temperature"] = trial.suggest_float("bagging_temperature", 0, 20)

    elif param["bootstrap_type"] == "Bernoulli":
        param["subsample"] = trial.suggest_float("subsample", 0.1, 1)


    clf = CatBoostRegressor(
        **param,
        thread_count=-1,
        random_seed=RANDOM_STATE,
    )

    clf.fit(
        X_train,
        y_train,
        eval_set=(X_val, y_val),
        verbose=0,
        plot=False,
        early_stopping_rounds=50,
    )

    y_pred = clf.predict(X_val)
    return clf, y_pred

def objective(trial, return_models=False):
    n_splits = 10
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)
    X_train = train_stack_df[cols_importance]
    y_train = y

    scores, models = [], []

    for train_idx, valid_idx in kf.split(X_train):
        train_data = X_train.iloc[train_idx, :], y_train.iloc[train_idx]
        valid_data = X_train.iloc[valid_idx, :], y_train.iloc[valid_idx]

        # –ü–æ–¥–∞–µ–º trials –¥–ª—è –ø–µ—Ä–µ–±–æ—Ä–∞
        model, y_pred = fit_st_with_feat_cb(trial, train_data, valid_data)
        scores.append(mean_squared_error(y_pred, valid_data[1], squared=False))
        models.append(model)
        break


    result = np.mean(scores)

    if return_models:
        return result, models
    else:
        return result

study = optuna.create_study(direction="minimize")
study.optimize(objective,
               n_trials=100,
               n_jobs = -1,
               show_progress_bar=True,)

valid_scores, optuna_st_with_feat_cb = objective(
    optuna.trial.FixedTrial(study.best_params),
    return_models=True,
)

valid_scores

# RMSE=11.9
pd.DataFrame({'car_id': car_id, 'target_reg': optuna_st_with_feat_cb[0].predict(test_stack_df[cols_importance]).reshape(-1)}).to_csv(f'optuna_st_with_feat_cb.csv', index=False)

"""#### Stacking DecisionTreeRegressor"""

def fit_st_with_feat_tree(trial, train, val):
    X_train, y_train = train
    X_val, y_val = val

    param = {"max_depth": trial.suggest_int("max_depth", 2, 10),
             "min_samples_split": trial.suggest_int("min_samples_split", 2, 10),
             "min_samples_leaf": trial.suggest_int("min_samples_leaf", 2, 8),
             "splitter": trial.suggest_categorical('splitter', ['best','random']),
             "max_features": trial.suggest_categorical("max_features", ['auto', 'sqrt', 'log2']),
             'criterion': trial.suggest_categorical('criterion', ['squared_error']),
             'random_state': RANDOM_STATE
             }

    clf = DecisionTreeRegressor(**param)

    clf.fit(X_train, y_train)

    y_pred = clf.predict(X_val)
    return clf, y_pred

def objective(trial, return_models=False):
    n_splits = 10
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)
    X_train = train_stack_df[cols_importance]
    y_train = y

    scores, models = [], []

    for train_idx, valid_idx in kf.split(X_train):
        train_data = X_train.iloc[train_idx, :], y_train.iloc[train_idx]
        valid_data = X_train.iloc[valid_idx, :], y_train.iloc[valid_idx]

        # –ü–æ–¥–∞–µ–º trials –¥–ª—è –ø–µ—Ä–µ–±–æ—Ä–∞
        model, y_pred = fit_st_with_feat_tree(trial, train_data, valid_data)
        scores.append(mean_squared_error(y_pred, valid_data[1], squared=False))
        models.append(model)
        break


    result = np.mean(scores)

    if return_models:
        return result, models
    else:
        return result

study = optuna.create_study(direction="minimize")
study.optimize(objective,
               n_trials=100,
               n_jobs = -1,
               show_progress_bar=True,)

valid_scores, optuna_st_with_feat_tree = objective(
    optuna.trial.FixedTrial(study.best_params),
    return_models=True,
)

valid_scores

# RMSE=12.2
pd.DataFrame({'car_id': car_id, 'target_reg': optuna_st_with_feat_tree[0].predict(test_stack_df[cols_importance]).reshape(-1)}).to_csv(f'optuna_st_with_feat_tree.csv', index=False)

"""#### Stacking RandomForestRegression"""

def fit_st_with_feat_rfr(trial, train, val):
    X_train, y_train = train
    X_val, y_val = val

    param = {
        'max_depth': trial.suggest_int('max_depth', 4, 10),
        'max_features': trial.suggest_categorical("max_features", ["auto", 'sqrt', 'log2']),
        'n_estimators': trial.suggest_int('n_estimators', 5, 400),
        'criterion': trial.suggest_categorical('criterion', ['squared_error']),
        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),
        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 2, 10),
        'random_state': RANDOM_STATE
    }

    clf = RandomForestRegressor(**param, n_jobs=-1)

    clf.fit(X_train, y_train)

    y_pred = clf.predict(X_val)
    return clf, y_pred

def objective(trial, return_models=False):
    n_splits = 10
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)
    X_train = train_stack_df[cols_importance]
    y_train = y

    scores, models = [], []

    for train_idx, valid_idx in kf.split(X_train):
        train_data = X_train.iloc[train_idx, :], y_train.iloc[train_idx]
        valid_data = X_train.iloc[valid_idx, :], y_train.iloc[valid_idx]

        # –ü–æ–¥–∞–µ–º trials –¥–ª—è –ø–µ—Ä–µ–±–æ—Ä–∞
        model, y_pred = fit_st_with_feat_rfr(trial, train_data, valid_data)
        scores.append(mean_squared_error(y_pred, valid_data[1], squared=False))
        models.append(model)
        break


    result = np.mean(scores)

    if return_models:
        return result, models
    else:
        return result

study = optuna.create_study(direction="minimize")
study.optimize(objective,
               n_trials=100,
               n_jobs = -1,
               show_progress_bar=True,)

valid_scores, optuna_st_with_feat_rfr = objective(
    optuna.trial.FixedTrial(study.best_params),
    return_models=True,
)

valid_scores

# RMSE=12.0
pd.DataFrame({'car_id': car_id, 'target_reg': optuna_st_with_feat_rfr[0].predict(test_stack_df[cols_importance])}).to_csv(f'optuna_st_with_feat_rfr.csv', index=False)

"""## CONCLUSION"""

# –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º (–º–æ–¥–µ–ª–∏ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –ª–∏–¥–µ—Ä–±–æ—Ä–¥–µ)
scores = pd.DataFrame()

scores['models'] = ['CatBoostRegressor_optuna','LGBMRegressor_optuna','XGBRegressor_optuna','LinearRegression_optuna',
                    'KNeighborsRegressor_optuna','SVR_optuna','RandomForestRegressor_optuna', 'DecisionTreeRegressor_optuna',
                    'CatBoostRegressor_base','LGBMRegressor_base','XGBRegressor_base','LinearRegression_base',
                    'KNeighborsRegressor_base','SVR_base','RandomForestRegressor_base', 'DecisionTreeRegressor_base','BlendingAllModelsMean',
                    'BlendingAllModelsMedian', 'Blending_Cb_Lgbm_Xgb_Weight', 'Blending_Rf_Cb_Xgb_Weight',
                    'StackingRegressor', 'StackingWithoutFeatCatboost', 'StackingWithoutFeatTree', 'StackingWithoutFeatRF',
                    'StackingWithFeatCatboost', 'StackingWithFeatTree', 'StackingWithFeatRF']

scores['leaderboard'] = [11.8, 11.9, 11.8, 19.5, 12.7, 12.6, 12.2, 12.4,
                         12.0, 12.4, 12.4, 18.7, 13.0, 15.0, 12.2, 16.7,
                         12.1, 11.8, 11.7, 11.7, 11.9, 11.8, 12.0, 12.0, 11.9, 12.2, 12.0]

bbox = {'boxstyle' : 'round',
        'pad' : 0.5,
        'facecolor' : 'white',
        'edgecolor' : 'black',
        'linewidth' : 1,
        'alpha' : 1}

plt.figure(figsize=(20, 8), dpi=150)
p1 = sns.barplot(scores.sort_values('leaderboard'), y='leaderboard', x='models', palette='viridis')
p1.bar_label(p1.containers[0], bbox = bbox)
plt.xticks(rotation=45, horizontalalignment='right');

"""**–í—ã–≤–æ–¥:**
- –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å CatBoostRegressor –ø–æ–∫–∞–∑–∞–ª–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç RMSE=12.0, —á—Ç–æ –¥–∞–ª–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏,—á—Ç–æ –ø—Ä–∏ —Ç—é–Ω–∏–Ω–≥–µ —É–≤–µ–ª–∏—á–∏–º —Å–∫–æ—Ä
- RandomForestRegressor –¥–∞–ª –ø–ª–æ—Ö–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç, —á—Ç–æ –≤ –±–∞–∑–µ, —á—Ç–æ —Å —Ç—é–Ω–∏–Ω–≥–æ–º –ø–æ–∫–∞–∑—ã–≤–∞–ª RMSE=12.2, –Ω–æ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –º–µ—Ç–∞ –º–æ–¥–µ–ª–∏ –ø—Ä–∏ StackingRegressor —É–≤–µ–ª–∏—á–∏–ª —Å–∫–æ—Ä –¥–æ RMSE=11.9, —É–∂–µ –ª—É—á—à–µ, –Ω–æ –≤—Å–µ –∂–µ –Ω–µ –¥–æ—Ç—è–≥–∏–≤–∞–ª –¥–æ –ø—Ä–æ–±–∏—Ç–∏—è –ø–æ—Ä–æ–≥–∞
- –ú–æ–¥–µ–ª–∏ CatBoostRegressor –∏ XGBRegressor –ø—Ä–∏ –ø–æ–º–æ—â–∏ –ø–æ–¥–±–æ—Ä–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–∞–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ RMSE=11.8, —á—Ç–æ –ø–æ–∑–≤–æ–ª–∏–ª–æ –µ—â–µ –≤ –Ω–∞—á–∞–ª–µ –ø—Ä–æ–±–∏—Ç—å –ø–æ—Ä–æ–≥ RMSE < 11.85
- –°—Ç–µ–∫–∏–Ω–≥ –Ω–µ –¥–∞–ª —É–ª—É—á—à–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞, –∞ –≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–ª—É—á–∞—è—Ö –¥–∞–∂–µ –±—ã–ª —Ö—É–∂–µ –±–∞–∑–æ–≤—ã—Ö –±—É—Å—Ç–∏–Ω–≥–æ–≤, –Ω–æ –∏—Å–ø–æ–ª—å–∑—É—è –≤ –∫–∞—á–µ—Å—Ç–≤–µ –º–µ—Ç–∞–º–æ–¥–µ–ª—å catboost –∏ –Ω–µ –≥–µ–Ω–µ—Ä–∏—Ä—É—è –Ω–æ–≤—ã–µ —Ñ–∏—á–∏, —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ª—É—á—à–∏–π —Å–∫–æ—Ä RMSE=11.8
- –ò—Å–ø–æ–ª—å–∑—É—è –≤ –∫–∞—á–µ—Å—Ç–≤–µ –º–µ—Ç–∞–º–æ–¥–µ–ª–µ–π RandomForestRegressor –∏ DecisionTreeRegressor –Ω–µ –¥–æ–±–∏–ª—Å—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∏ —Å—É–¥—è –ø–æ —Å–∫–æ—Ä—É –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–æ–≤—ã—Ö —Ñ–∏—á–µ–π –Ω–∏—á–µ–º –Ω–µ –ø–æ–º–æ–≥–ª–∞, –∞ –¥–∞–∂–µ —É—Ö—É–¥—à–∏–ª–∞ —Å–∫–æ—Ä
- –ê –≤–æ—Ç –±–ª–µ–Ω–¥–∏–Ω–≥ –ø—Ä–∏ –ø–æ–º–æ—â–∏ –ø–æ–¥–±–æ—Ä–∞ –≤–µ—Å–æ–≤ –ø–æ–∑–≤–æ–ª–∏–ª —É–ª—É—á—à–∏—Ç—å —Å–∫–æ—Ä, –ø—Ä–∏—á–µ–º –ø—Ä–∏ –±–ª–µ–Ω–¥–∏–Ω–≥–µ —Å—Ä–∞–∑—É 3 –±—É—Å—Ç–∏–Ω–≥–æ–≤ —Å–∫–æ—Ä –±—ã–ª —Ä–∞–≤–µ–Ω –±–ª–µ–Ω–¥–∏–Ω–≥—É 2-—Ö –±—É—Å—Ç–∏–Ω–≥–æ–≤ –∏ 1-–æ–π –º–æ–¥–µ–ª–∏ —Å–ª—É—á–∞–π–Ω–æ–≥–æ –ª–µ—Å–∞, –∞ –∏–º–µ–Ω–Ω–æ —Å–∫–æ—Ä —Å–æ—Å—Ç–∞–≤–∏–ª RMSE=11.7
- –ë–ª–µ–Ω–¥–∏–Ω–≥ –±–µ–∑ –ø–æ–¥–±–æ—Ä–∞ –≤–µ—Å–æ–≤ –Ω–µ —É–ª—É—á—à–∞–ª —Å–∫–æ—Ä, –Ω–æ –∏ –Ω–µ —É—Ö—É–¥—à–∞–ª, –∞ —Å–æ—Ö—Ä–∞–Ω—è–ª –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø—Ä–æ–±–∏—Ç–∏—è –ø–æ—Ä–æ–≥–∞ RMSE=11.8
- –ù–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –≤—ã—à–µ–∏–∑–ª–æ–∂–µ–Ω–Ω–æ–≥–æ, –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤—ã–±–∏—Ä–∞—é –º–æ–¥–µ–ª—å Blending_Rf_Cb_Xgb_Weight
"""

pd.read_csv('weight_rf_cb_xgb.csv')

